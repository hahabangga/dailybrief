name: Nightly news fetch

on:
  schedule:
    # 매일 00:05 UTC (한국 09:05 KST 즈음)
    - cron: "5 0 * * *"
  workflow_dispatch:

permissions:
  contents: write

jobs:
  fetch-news:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with: { fetch-depth: 0 }

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install deps
        run: |
          python -m pip install --upgrade pip
          pip install feedparser beautifulsoup4

      - name: Build summarized news JSON
        run: |
          python - << 'PY'
          import os, json, re, time
          from datetime import datetime, timezone
          from urllib.parse import quote
          import feedparser
          from bs4 import BeautifulSoup

          def clean_html(txt):
              if not txt: return ''
              soup = BeautifulSoup(txt, 'html.parser')
              t = soup.get_text(' ', strip=True)
              t = re.sub(r'\s+', ' ', t)
              # 구글뉴스 RSS 끝의 ' - 매체명' 제거
              t = re.sub(r'\s+-\s+[^-]{2,50}$', '', t)
              return t

          def first_sentence(s, limit=200):
              if not s: return ''
              s = s.strip()
              # 문장 끝 기준으로 잘라서 1~2문장
              parts = re.split(r'(?<=[.!?])\s+', s)
              if len(parts) >= 2:
                  out = ' '.join(parts[:2])
              else:
                  out = parts[0]
              return (out[:limit] + '…') if len(out) > limit else out

          def fetch_section(query, lang='en-US', gl='US', ceid='US:en', max_items=5):
              url = f"https://news.google.com/rss/search?q={quote(query)}&hl={lang}&gl={gl}&ceid={ceid}"
              d = feedparser.parse(url)
              items = []
              for e in d.entries[:max_items]:
                  title = clean_html(getattr(e, 'title', ''))
                  link  = getattr(e, 'link', '')
                  desc  = clean_html(getattr(e, 'summary', ''))
                  pub   = None
                  if getattr(e, 'published_parsed', None):
                      pub = datetime.fromtimestamp(time.mktime(e.published_parsed), tz=timezone.utc).isoformat().replace('+00:00','Z')
                  summary = first_sentence(desc or title, limit=220)
                  items.append({"title": title, "url": link, "published": pub, "summary": summary})
              return items

          sections = {
              "us_stock":   fetch_section("US stock market OR S&P 500 OR Dow Jones OR Nasdaq"),
              "us_econ":    fetch_section("US economy CPI inflation jobs PCE"),
              "us_politics":fetch_section("US politics Congress White House election"),
              "world":      fetch_section("geopolitics OR war OR conflict OR politics OR entertainment", lang='ko', gl='KR', ceid='KR:ko'),
          }

          out = {
              "generated_at": datetime.now(timezone.utc).isoformat().replace('+00:00','Z'),
              "sections": sections
          }

          os.makedirs("data", exist_ok=True)
          with open("data/news.json", "w", encoding="utf-8") as f:
              json.dump(out, f, ensure_ascii=False, indent=2)
          PY

      - name: Commit & push if changed
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git add -A
          if git diff --cached --quiet; then
            echo "No changes to commit"
          else
            git commit -m "chore(data): update news summaries [skip ci]"
            git push
          fi
