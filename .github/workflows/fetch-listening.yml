name: Fetch CNN listening (06:30 & 12:30 KST, hardened)

on:
  schedule:
    - cron: "30 21 * * *"  # 06:30 KST
    - cron: "30 3 * * *"   # 12:30 KST
  workflow_dispatch:

permissions:
  contents: write

jobs:
  fetch-listening:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        with: { fetch-depth: 0 }

      - uses: actions/setup-python@v5
        with: { python-version: '3.11' }

      - name: Install deps
        run: |
          python -m pip install --upgrade pip
          pip install feedparser beautifulsoup4 lxml yake

      - name: Build data/listening.json (CNN 5 Things → NPR Up First fallback)
        run: |
          python - << 'PY'
          import os, re, json, datetime
          import feedparser
          from bs4 import BeautifulSoup
          from zoneinfo import ZoneInfo
          import yake

          KST = ZoneInfo("Asia/Seoul")
          SOURCES = [
            ("CNN 5 Things", "https://rss.art19.com/cnn-5-things"),
            ("NPR Up First", "https://feeds.npr.org/510318/podcast.xml"),
          ]

          def clean_html(txt):
            if not txt: return ''
            soup = BeautifulSoup(txt, 'html.parser')
            t = soup.get_text(' ', strip=True)
            return re.sub(r'\s+', ' ', t).strip()

          def sentence_split(text):
            if not text: return []
            s = re.split(r'(?<=[\.\?\!])\s+', text)
            out = []
            for x in s:
              x = x.strip()
              if len(x.split()) >= 5 and len(x) <= 220:
                out.append(x)
            return out

          def top_keywords(text, topk=6):
            try:
              ex = yake.KeywordExtractor(lan="en", n=1, top=topk)
              kws = ex.extract_keywords(text)
              cand = [k for k,_ in kws if re.match(r"^[A-Za-z][A-Za-z\-']+$", k)]
              seen=set(); out=[]
              stop = set('the a an to for of in on at by and or with from into about over after before under between among as is are be was were has have had will would can could should may might do does did not no'.split())
              for w in cand:
                lw=w.lower()
                if lw in seen or lw in stop: continue
                seen.add(lw); out.append(w)
                if len(out)>=topk: break
              return out
            except Exception:
              return []

          def pick_first_with_audio(feed_url):
            d = feedparser.parse(feed_url)
            for e in d.entries:
              # mp3 찾기
              audio_url = ""
              if getattr(e, 'enclosures', None):
                for enc in e.enclosures:
                  u = enc.get('href') or enc.get('url') or ''
                  if u.lower().endswith('.mp3'):
                    audio_url = u; break
              # 대체: media_content
              if not audio_url and getattr(e, 'media_content', None):
                for mc in e.media_content:
                  u = mc.get('url','')
                  if u.lower().endswith('.mp3'): audio_url=u; break
              # 설명 후보 모으기
              descs = []
              for key in ('summary','subtitle','itunes_summary','description'):
                v = getattr(e, key, '') or ''
                if v: descs.append(v)
              # content:encoded
              if getattr(e, 'content', None):
                for c in e.content:
                  if 'value' in c and c['value']:
                    descs.append(c['value'])
              text = clean_html(' '.join(descs))
              title = clean_html(getattr(e,'title','(no title)'))
              link  = getattr(e,'link','')
              # 발행시각
              pub_iso = None
              tm = getattr(e,'published_parsed',None) or getattr(e,'updated_parsed',None)
              if tm:
                pub_utc = datetime.datetime(*tm[:6], tzinfo=datetime.timezone.utc)
                pub_iso = pub_utc.isoformat().replace('+00:00','Z')

              # 문장 뽑기(없으면 title+desc)
              sents = sentence_split(text)
              if len(sents) < 8:
                sents = sentence_split(f"{title}. {text}")
              sents = sents[:16]

              if audio_url and sents:
                return {
                  "episode_title": title, "source_link": link, "audio_url": audio_url,
                  "published_utc": pub_iso, "script_en": sents
                }
            return None

          payload = None; src_name=None
          for name, url in SOURCES:
            item = pick_first_with_audio(url)
            if item:
              src_name = name; payload = item; break

          if not payload:
            raise SystemExit("No playable episode with sentences found.")

          keywords = top_keywords(' '.join(payload["script_en"])) if payload["script_en"] else []
          out = {
            "date": datetime.datetime.now(KST).date().isoformat(),
            "source": src_name,
            **payload,
            "keywords_en": keywords
          }

          os.makedirs("data", exist_ok=True)
          with open("data/listening.json","w",encoding="utf-8") as f:
            json.dump(out, f, ensure_ascii=False, indent=2)

          print("OK:", out["source"], "→", out["episode_title"])
          print("audio_url:", out["audio_url"])
          print("sentences:", len(out["script_en"]))
          PY

      - name: Commit & push
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git add -A
          git commit -m "chore(listening): robust fetch (CNN→NPR fallback) [skip ci]" || echo "No changes"
          git push || true
